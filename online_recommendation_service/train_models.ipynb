{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "import mlflow\n",
    "from datetime import datetime\n",
    "\n",
    "from src.modules.graph_nn.dataset import TransactionDataset\n",
    "from src.modules.graph_nn.model import TransactionGNN\n",
    "from src.modules.graph_nn.train import train_epoch, evaluate_epoch\n",
    "\n",
    "\n",
    "class Constants:\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    BATCH_SIZE = 32\n",
    "    HIDDEN_DIM = 256\n",
    "    NUM_LAYERS = 5\n",
    "    NUM_HEADS = 4\n",
    "    SHUFFLE = False\n",
    "    TRAIN_SPLIT = 0.8\n",
    "\n",
    "    TRANSACTIONS_PATH = \"/Users/alfa/Documents/diplom/graphnn-recommendation-system/data/processed_transactions_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch:: 100%|██████████| 139/139 [00:02<00:00, 66.62it/s]\n",
      "Evaluate Epoch:: 100%|██████████| 35/35 [00:00<00:00, 266.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train Loss: 4.4518, Test Acc: 0.0275, Test Loss: 4.5033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch:: 100%|██████████| 139/139 [00:01<00:00, 76.05it/s]\n",
      "Evaluate Epoch:: 100%|██████████| 35/35 [00:00<00:00, 288.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 4.1285, Test Acc: 0.0009, Test Loss: 4.9181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch:: 100%|██████████| 139/139 [00:01<00:00, 79.65it/s]\n",
      "Evaluate Epoch:: 100%|██████████| 35/35 [00:00<00:00, 270.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Train Loss: 4.0917, Test Acc: 0.0055, Test Loss: 4.8772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch:: 100%|██████████| 139/139 [00:01<00:00, 76.40it/s]\n",
      "Evaluate Epoch:: 100%|██████████| 35/35 [00:00<00:00, 236.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Train Loss: 4.1012, Test Acc: 0.0183, Test Loss: 4.4578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch:: 100%|██████████| 139/139 [00:01<00:00, 80.64it/s]\n",
      "Evaluate Epoch:: 100%|██████████| 35/35 [00:00<00:00, 301.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Train Loss: 4.0227, Test Acc: 0.0046, Test Loss: 4.7484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Загружаем транзакции и создаем torch dataset\n",
    "transactions = pd.read_parquet(Constants.TRANSACTIONS_PATH)\n",
    "graph_dataset = TransactionDataset(transactions)\n",
    "\n",
    "# Разделяем на train и test\n",
    "graph_train_size = int(Constants.TRAIN_SPLIT * len(graph_dataset))\n",
    "graph_train_dataset = torch.utils.data.Subset(graph_dataset, range(graph_train_size))\n",
    "graph_test_dataset = torch.utils.data.Subset(\n",
    "    graph_dataset, range(graph_train_size, len(graph_dataset))\n",
    ")\n",
    "\n",
    "# Создаем загрузчики данных\n",
    "graph_train_loader = DataLoader(\n",
    "    graph_train_dataset, batch_size=Constants.BATCH_SIZE, shuffle=Constants.SHUFFLE\n",
    ")\n",
    "graph_test_loader = DataLoader(\n",
    "    graph_test_dataset, batch_size=Constants.BATCH_SIZE, shuffle=Constants.SHUFFLE\n",
    ")\n",
    "\n",
    "# Инициализируем модель\n",
    "graph_model = TransactionGNN(\n",
    "    num_items=graph_dataset.num_items,\n",
    "    hidden_dim=Constants.HIDDEN_DIM,\n",
    "    num_layers=Constants.NUM_LAYERS,\n",
    "    num_heads=Constants.NUM_HEADS,\n",
    ").to(Constants.DEVICE)\n",
    "graph_optimizer = torch.optim.Adam(graph_model.parameters(), lr=0.001)\n",
    "\n",
    "# Обучение и оценка\n",
    "for epoch in range(5):\n",
    "    train_loss = train_epoch(\n",
    "        model=graph_model,\n",
    "        loader=graph_train_loader,\n",
    "        optimizer=graph_optimizer,\n",
    "        device=Constants.DEVICE,\n",
    "    )\n",
    "    test_acc, test_loss = evaluate_epoch(\n",
    "        model=graph_model,\n",
    "        loader=graph_test_loader,\n",
    "        device=Constants.DEVICE,\n",
    "    )\n",
    "    print(\n",
    "        f\"Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Test Acc: {test_acc:.4f}, Test Loss: {test_loss:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логирование артефактов моделей в MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minio123\"\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://localhost:9000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/18 19:33:39 INFO mlflow.tracking.fluent: Experiment with name 'graph-nn-model' does not exist. Creating a new experiment.\n",
      "\u001b[31m2025/04/18 19:33:43 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run model_20250418_193339 at: http://localhost:5000/#/experiments/1/runs/a69b307154ae4f2fb88e140355420935\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "def save_model_to_mlflow():\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "    mlflow.set_experiment(\"graph-nn-model\")\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"model_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"):\n",
    "        # Сохраняем модель PyTorch\n",
    "        mlflow.pytorch.log_model(graph_model, \"model\")\n",
    "\n",
    "        # Сохраняем энкодер товаров\n",
    "        import joblib\n",
    "\n",
    "        joblib.dump(graph_dataset.item_encoder, \"item_encoder.pkl\")\n",
    "        mlflow.log_artifact(\"item_encoder.pkl\", \"encoders\")\n",
    "\n",
    "        # Логируем параметры модели\n",
    "        mlflow.log_params(\n",
    "            {\n",
    "                \"num_items\": graph_dataset.num_items,\n",
    "                \"hidden_dim\": Constants.HIDDEN_DIM,\n",
    "                \"num_layers\": Constants.NUM_LAYERS,\n",
    "                \"num_heads\": Constants.NUM_HEADS,\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "save_model_to_mlflow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
