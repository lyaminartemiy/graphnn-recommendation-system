{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19c5c022",
   "metadata": {},
   "source": [
    "# Исследование моделей рекомендаций"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6535ad03",
   "metadata": {},
   "source": [
    "## Подготовка данных для обучения и тестов (global time split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3178bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Constants:\n",
    "    USER_ID = \"user_id\"\n",
    "    ITEM_ID = \"item_id\"\n",
    "    TIMESTAMP = \"time\"\n",
    "\n",
    "    TRANSACTIONS_PATH = \"/Users/alfa/Documents/diplom/graphnn-recommendation-system/data/transactions_train.csv\"\n",
    "    CUSTOMERS_PATH = \"/Users/alfa/Documents/diplom/graphnn-recommendation-system/data/customers.csv\"\n",
    "    ARTICLES_PATH = \"/Users/alfa/Documents/diplom/graphnn-recommendation-system/data/articles.csv\"\n",
    "\n",
    "    RESULT_TRANSACTIONS_PATH = \"/Users/alfa/Documents/diplom/graphnn-recommendation-system/data/processed_transactions_train.csv\"\n",
    "    RESULT_CUSTOMERS_PATH = \"/Users/alfa/Documents/diplom/graphnn-recommendation-system/data/processed_customers_train.csv\"\n",
    "    RESULT_ARTICLES_PATH = \"/Users/alfa/Documents/diplom/graphnn-recommendation-system/data/processed_articles_train.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2430bff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_balanced_dataset(\n",
    "    transactions: pd.DataFrame,\n",
    "    articles: pd.DataFrame,\n",
    "    num_articles_per_type: int = 50,\n",
    "    num_customers: int = 500,\n",
    "    min_articles_per_user: int = 5,\n",
    "    min_users_per_article: int = 5,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Формирует сбалансированный датасет:\n",
    "    - одинаковое количество артикулов на каждый product_type_name (если возможно)\n",
    "    - одинаковое количество взаимодействий с каждым product_type_name\n",
    "    - отбирает пользователей с минимум min_articles_per_user покупками\n",
    "    \"\"\"\n",
    "\n",
    "    # Считаем популярность товаров\n",
    "    article_popularity = transactions['item_id'].value_counts()\n",
    "    popular_articles = article_popularity[article_popularity >= min_users_per_article].index\n",
    "\n",
    "    # Фильтруем articles по популярным\n",
    "    filtered_articles = articles[articles['item_id'].isin(popular_articles)]\n",
    "\n",
    "    # Оставляем только те product_type_name, где достаточно товаров\n",
    "    type_counts = filtered_articles['product_type_name'].value_counts()\n",
    "    eligible_types = type_counts[type_counts >= num_articles_per_type].index\n",
    "\n",
    "    print(f\"Всего типов с достаточным числом товаров: {len(eligible_types)}\")\n",
    "\n",
    "    # Формируем финальный список артикулов по каждому типу\n",
    "    selected_items = []\n",
    "    for pt in eligible_types:\n",
    "        items_in_type = filtered_articles[filtered_articles['product_type_name'] == pt]['item_id'].sample(\n",
    "            num_articles_per_type, random_state=42\n",
    "        ).tolist()\n",
    "        selected_items.extend(items_in_type)\n",
    "\n",
    "    # Фильтруем транзакции по этим артиклам\n",
    "    filtered_data = transactions[transactions['item_id'].isin(selected_items)]\n",
    "\n",
    "    # Считаем покупки по пользователям\n",
    "    user_purchase_counts = filtered_data['user_id'].value_counts()\n",
    "\n",
    "    # Отбираем пользователей с минимум min_articles_per_user покупками\n",
    "    eligible_users = user_purchase_counts[user_purchase_counts >= min_articles_per_user].index\n",
    "\n",
    "    # Берём случайные num_customers из eligible_users\n",
    "    selected_users = pd.Series(eligible_users).sample(\n",
    "        min(num_customers, len(eligible_users)), random_state=42\n",
    "    ).tolist()\n",
    "\n",
    "    # Финальный датасет\n",
    "    final_data = filtered_data[filtered_data['user_id'].isin(selected_users)].reset_index(drop=True)\n",
    "\n",
    "    print(f\"Всего отобрано product_type_name: {len(eligible_types)}\")\n",
    "    print(f\"Отобрано товаров: {len(selected_items)}\")\n",
    "    print(f\"Отобрано пользователей: {len(selected_users)}\")\n",
    "    print(f\"Финальный размер данных: {len(final_data)} транзакций\")\n",
    "\n",
    "    return final_data, selected_users, selected_items, eligible_types.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa209c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.read_csv(Constants.TRANSACTIONS_PATH)\n",
    "transactions = transactions.rename(\n",
    "    columns={\n",
    "        \"t_dat\": Constants.TIMESTAMP,\n",
    "        \"customer_id\": Constants.USER_ID,\n",
    "        \"article_id\": Constants.ITEM_ID,\n",
    "    }\n",
    ")\n",
    "transactions[Constants.USER_ID] = transactions[Constants.USER_ID].astype(str)\n",
    "transactions[Constants.ITEM_ID] = transactions[Constants.ITEM_ID].astype(str)\n",
    "\n",
    "customers = pd.read_csv(Constants.CUSTOMERS_PATH)\n",
    "customers = (\n",
    "    customers.rename(\n",
    "        columns={\n",
    "            \"customer_id\": Constants.USER_ID,\n",
    "            \"Active\": \"is_active\",\n",
    "            \"age\": \"age\",\n",
    "        }\n",
    "    )\n",
    "    .drop(columns=[\"FN\", \"postal_code\"])\n",
    ")\n",
    "customers[Constants.USER_ID] = customers[Constants.USER_ID].astype(str)\n",
    "\n",
    "articles = pd.read_csv(Constants.ARTICLES_PATH)\n",
    "articles = (\n",
    "    articles.rename(\n",
    "        columns={\"article_id\": Constants.ITEM_ID}\n",
    "    )\n",
    "    [[\n",
    "        Constants.ITEM_ID,\n",
    "        \"prod_name\",\n",
    "        \"product_type_name\",\n",
    "        \"product_group_name\",\n",
    "        \"colour_group_name\",\n",
    "        \"detail_desc\",\n",
    "    ]]\n",
    ")\n",
    "articles[Constants.ITEM_ID] = articles[Constants.ITEM_ID].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec8b4fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего типов с достаточным числом товаров: 90\n",
      "Всего отобрано product_type_name: 90\n",
      "Отобрано товаров: 900\n",
      "Отобрано пользователей: 500\n",
      "Финальный размер данных: 1897 транзакций\n"
     ]
    }
   ],
   "source": [
    "filtered_transactions, selected_users, selected_items, eligible_types = prepare_balanced_dataset(\n",
    "    transactions=transactions,\n",
    "    articles=articles,\n",
    "    num_articles_per_type=10,  # Увеличиваем для большего разнообразия\n",
    "    num_customers=500,       # Целевое количество пользователей\n",
    "    min_articles_per_user=3,   # Минимум 5 покупок на пользователя\n",
    "    min_users_per_article=10   # Минимум 10 пользователей на товар\n",
    ")\n",
    "\n",
    "filtered_customers = customers[customers[Constants.USER_ID].isin(selected_users)]\n",
    "filtered_customers = filtered_customers.reset_index(drop=True)\n",
    "filtered_articles = articles[articles[Constants.ITEM_ID].isin(selected_items)]\n",
    "filtered_articles = filtered_articles.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9c4ccc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>prod_name</th>\n",
       "      <th>product_type_name</th>\n",
       "      <th>product_group_name</th>\n",
       "      <th>colour_group_name</th>\n",
       "      <th>detail_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>130035001</td>\n",
       "      <td>Black Umbrella</td>\n",
       "      <td>Umbrella</td>\n",
       "      <td>Items</td>\n",
       "      <td>Black</td>\n",
       "      <td>Umbrella with a telescopic handle and matching...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156224002</td>\n",
       "      <td>Box 4p Socks</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Black</td>\n",
       "      <td>Semi-matte socks with a short shaft. 20 denier.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>174057038</td>\n",
       "      <td>FLEECE PYJAMA</td>\n",
       "      <td>Pyjama jumpsuit/playsuit</td>\n",
       "      <td>Nightwear</td>\n",
       "      <td>Light Pink</td>\n",
       "      <td>All-in-one pyjamas in soft, patterned fleece t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>187949028</td>\n",
       "      <td>Padded pyjama</td>\n",
       "      <td>Pyjama jumpsuit/playsuit</td>\n",
       "      <td>Nightwear</td>\n",
       "      <td>Dark Blue</td>\n",
       "      <td>Lightly padded all-in-one pyjamas in soft cott...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>188183015</td>\n",
       "      <td>Spanx alot Swimsuit</td>\n",
       "      <td>Swimsuit</td>\n",
       "      <td>Swimwear</td>\n",
       "      <td>Dark Green</td>\n",
       "      <td>Fully lined shaping swimsuit that has a sculpt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     item_id            prod_name         product_type_name  \\\n",
       "0  130035001       Black Umbrella                  Umbrella   \n",
       "1  156224002         Box 4p Socks                   Unknown   \n",
       "2  174057038        FLEECE PYJAMA  Pyjama jumpsuit/playsuit   \n",
       "3  187949028        Padded pyjama  Pyjama jumpsuit/playsuit   \n",
       "4  188183015  Spanx alot Swimsuit                  Swimsuit   \n",
       "\n",
       "  product_group_name colour_group_name  \\\n",
       "0              Items             Black   \n",
       "1            Unknown             Black   \n",
       "2          Nightwear        Light Pink   \n",
       "3          Nightwear         Dark Blue   \n",
       "4           Swimwear        Dark Green   \n",
       "\n",
       "                                         detail_desc  \n",
       "0  Umbrella with a telescopic handle and matching...  \n",
       "1    Semi-matte socks with a short shaft. 20 denier.  \n",
       "2  All-in-one pyjamas in soft, patterned fleece t...  \n",
       "3  Lightly padded all-in-one pyjamas in soft cott...  \n",
       "4  Fully lined shaping swimsuit that has a sculpt...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bace90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-09-21</td>\n",
       "      <td>c492583641b998d7bb4362933c02064d619eeceff3b8a4...</td>\n",
       "      <td>665095002</td>\n",
       "      <td>0.025407</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-09-23</td>\n",
       "      <td>c8c7e97d83e298c32742a9f584a90f270561566c52d849...</td>\n",
       "      <td>640807002</td>\n",
       "      <td>0.067780</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-09-24</td>\n",
       "      <td>1f56cb4d9cd7468412c6dac613ba84da67adbaa981247e...</td>\n",
       "      <td>293433001</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-09-24</td>\n",
       "      <td>1f56cb4d9cd7468412c6dac613ba84da67adbaa981247e...</td>\n",
       "      <td>293433001</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-09-24</td>\n",
       "      <td>1f56cb4d9cd7468412c6dac613ba84da67adbaa981247e...</td>\n",
       "      <td>293433001</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time                                            user_id    item_id  \\\n",
       "0  2018-09-21  c492583641b998d7bb4362933c02064d619eeceff3b8a4...  665095002   \n",
       "1  2018-09-23  c8c7e97d83e298c32742a9f584a90f270561566c52d849...  640807002   \n",
       "2  2018-09-24  1f56cb4d9cd7468412c6dac613ba84da67adbaa981247e...  293433001   \n",
       "3  2018-09-24  1f56cb4d9cd7468412c6dac613ba84da67adbaa981247e...  293433001   \n",
       "4  2018-09-24  1f56cb4d9cd7468412c6dac613ba84da67adbaa981247e...  293433001   \n",
       "\n",
       "      price  sales_channel_id  \n",
       "0  0.025407                 1  \n",
       "1  0.067780                 2  \n",
       "2  0.016932                 2  \n",
       "3  0.016932                 2  \n",
       "4  0.016932                 2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2d1e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_dataset(transactions: pd.DataFrame, articles: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Дополнительная обработка датасета для улучшения рекомендаций\"\"\"\n",
    "    \n",
    "    # Добавляем информацию о товарах\n",
    "    enriched_data = transactions.merge(\n",
    "        articles[[Constants.ITEM_ID, 'product_type_name', 'product_group_name']],\n",
    "        on=Constants.ITEM_ID,\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Преобразуем временные метки\n",
    "    enriched_data[Constants.TIMESTAMP] = pd.to_datetime(enriched_data[Constants.TIMESTAMP])\n",
    "    enriched_data['days_since_epoch'] = (enriched_data[Constants.TIMESTAMP] - pd.Timestamp('2018-01-01')).dt.days\n",
    "    \n",
    "    # Добавляем сезонность\n",
    "    enriched_data['month'] = enriched_data[Constants.TIMESTAMP].dt.month\n",
    "    enriched_data['season'] = enriched_data['month'] % 12 // 3 + 1\n",
    "    \n",
    "    # Удаляем дубликаты (одинаковые покупки одного товара одним пользователем в один день)\n",
    "    enriched_data = enriched_data.drop_duplicates(\n",
    "        subset=[Constants.USER_ID, Constants.ITEM_ID, Constants.TIMESTAMP]\n",
    "    )\n",
    "    \n",
    "    return enriched_data\n",
    "\n",
    "enhanced_transactions = enhance_dataset(filtered_transactions, filtered_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da29e9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_based_split(data: pd.DataFrame, \n",
    "                    user_col: str = Constants.USER_ID,\n",
    "                    time_col: str = Constants.TIMESTAMP,\n",
    "                    test_ratio: float = 0.2) -> tuple:\n",
    "    \"\"\"Глобальное временное разделение с сохранением последовательности для каждого пользователя\"\"\"\n",
    "    \n",
    "    # Сортируем по пользователю и времени\n",
    "    data = data.sort_values([user_col, time_col])\n",
    "    \n",
    "    # Вычисляем точку разделения для каждого пользователя\n",
    "    def split_user_group(df):\n",
    "        split_idx = int(len(df) * (1 - test_ratio))\n",
    "        train = df.iloc[:split_idx]\n",
    "        test = df.iloc[split_idx:]\n",
    "        return train, test\n",
    "    \n",
    "    # Группируем и разделяем\n",
    "    grouped = data.groupby(user_col, group_keys=False)\n",
    "    train_data = grouped.apply(lambda x: split_user_group(x)[0])\n",
    "    test_data = grouped.apply(lambda x: split_user_group(x)[1])\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "train_data, test_data = time_based_split(enhanced_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cd553fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1018, 10), (518, 10))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "428c9b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "class RecommendationMetricsDf:\n",
    "    def __init__(self, recommendations_df, history_df, USER_ID, ITEM_ID):\n",
    "        \"\"\"\n",
    "        recommendations_df: pd.DataFrame с колонками [USER_ID, ITEM_ID, 'score']\n",
    "        history_df: pd.DataFrame с колонками [USER_ID, ITEM_ID]\n",
    "        \"\"\"\n",
    "        self.recommendations_df = recommendations_df\n",
    "        self.history_df = history_df\n",
    "        self.USER_ID = USER_ID\n",
    "        self.ITEM_ID = ITEM_ID\n",
    "\n",
    "    def precision_at_k(self, k=10):\n",
    "        precisions = []\n",
    "\n",
    "        for user_id in self.recommendations_df[self.USER_ID].unique():\n",
    "            recs = self.recommendations_df[self.recommendations_df[self.USER_ID] == user_id].sort_values('score', ascending=False).head(k)\n",
    "            actual = set(self.history_df[self.history_df[self.USER_ID] == user_id][self.ITEM_ID])\n",
    "            recommended = set(recs[self.ITEM_ID])\n",
    "\n",
    "            if len(recommended) > 0:\n",
    "                precision = len(actual & recommended) / len(recommended)\n",
    "            else:\n",
    "                precision = 0.0\n",
    "\n",
    "            precisions.append(precision)\n",
    "\n",
    "        return np.mean(precisions)\n",
    "\n",
    "    def recall_at_k(self, k=10):\n",
    "        recalls = []\n",
    "\n",
    "        for user_id in self.recommendations_df[self.USER_ID].unique():\n",
    "            recs = self.recommendations_df[self.recommendations_df[self.USER_ID] == user_id].sort_values('score', ascending=False).head(k)\n",
    "            actual = set(self.history_df[self.history_df[self.USER_ID] == user_id][self.ITEM_ID])\n",
    "            recommended = set(recs[self.ITEM_ID])\n",
    "\n",
    "            if len(actual) > 0:\n",
    "                recall = len(actual & recommended) / len(actual)\n",
    "            else:\n",
    "                recall = 0.0\n",
    "\n",
    "            recalls.append(recall)\n",
    "\n",
    "        return np.mean(recalls)\n",
    "\n",
    "    def map_at_k(self, k=10):\n",
    "        aps = []\n",
    "\n",
    "        for user_id in self.recommendations_df[self.USER_ID].unique():\n",
    "            recs = self.recommendations_df[self.recommendations_df[self.USER_ID] == user_id].sort_values('score', ascending=False).head(k)\n",
    "            actual = set(self.history_df[self.history_df[self.USER_ID] == user_id][self.ITEM_ID])\n",
    "            recommended = list(recs[self.ITEM_ID])\n",
    "\n",
    "            if not actual:\n",
    "                aps.append(0.0)\n",
    "                continue\n",
    "\n",
    "            precision_sum = 0.0\n",
    "            num_hits = 0\n",
    "\n",
    "            for i, item in enumerate(recommended, 1):\n",
    "                if item in actual:\n",
    "                    num_hits += 1\n",
    "                    precision_sum += num_hits / i\n",
    "\n",
    "            aps.append(precision_sum / min(len(actual), k))\n",
    "\n",
    "        return np.mean(aps)\n",
    "\n",
    "    def ndcg_at_k(self, k=10):\n",
    "        ndcgs = []\n",
    "\n",
    "        for user_id in self.recommendations_df[self.USER_ID].unique():\n",
    "            recs = self.recommendations_df[self.recommendations_df[self.USER_ID] == user_id].sort_values('score', ascending=False).head(k)\n",
    "            actual_items = set(self.history_df[self.history_df[self.USER_ID] == user_id][self.ITEM_ID])\n",
    "            recommended_items = list(recs[self.ITEM_ID])\n",
    "\n",
    "            if not actual_items:\n",
    "                ndcgs.append(0.0)\n",
    "                continue\n",
    "\n",
    "            true_relevance = np.array([1 if item in actual_items else 0 for item in recommended_items])\n",
    "            pred_scores = np.array(recs['score'])\n",
    "\n",
    "            if true_relevance.sum() == 0:\n",
    "                ndcgs.append(0.0)\n",
    "            else:\n",
    "                ndcgs.append(ndcg_score([true_relevance], [pred_scores]))\n",
    "\n",
    "        return np.mean(ndcgs)\n",
    "\n",
    "    def coverage(self, total_items):\n",
    "        recommended_items = set(self.recommendations_df[self.ITEM_ID])\n",
    "        return len(recommended_items) / total_items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96453209",
   "metadata": {},
   "source": [
    "# ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad0dee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_ID = \"user_id\"\n",
    "ITEM_ID = \"item_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9355c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from implicit.nearest_neighbours import bm25_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import scipy.sparse as sparse\n",
    "import numpy as np\n",
    "\n",
    "class ALSRecommender:\n",
    "    def __init__(self, factors=100, iterations=15, regularization=0.01, alpha=40):\n",
    "        self.factors = factors\n",
    "        self.iterations = iterations\n",
    "        self.regularization = regularization\n",
    "        self.alpha = alpha  # Коэффициент доверия для неявных отзывов\n",
    "        self.model = None\n",
    "        self.user_encoder = LabelEncoder()\n",
    "        self.item_encoder = LabelEncoder()\n",
    "    \n",
    "    def prepare_data(self, transactions, USER_ID, ITEM_ID):\n",
    "        \"\"\"Подготовка данных в формат user-item матрицы\"\"\"\n",
    "        user_ids = self.user_encoder.fit_transform(transactions[USER_ID])\n",
    "        item_ids = self.item_encoder.fit_transform(transactions[ITEM_ID])\n",
    "        \n",
    "        # Создаем CSR матрицу сразу\n",
    "        user_item_matrix = sparse.csr_matrix(\n",
    "            (np.ones(len(transactions)), (user_ids, item_ids)),\n",
    "            shape=(len(self.user_encoder.classes_), len(self.item_encoder.classes_))\n",
    "        )\n",
    "        \n",
    "        # Взвешивание BM25 (вернет CSR матрицу)\n",
    "        weighted_matrix = bm25_weight(user_item_matrix, K1=100, B=0.8)\n",
    "        \n",
    "        # Убедимся, что матрица в CSR формате\n",
    "        return weighted_matrix.tocsr() if not sparse.isspmatrix_csr(weighted_matrix) else weighted_matrix\n",
    "\n",
    "    def train(self, train_data, test_data, USER_ID, ITEM_ID):\n",
    "        \"\"\"Обучение модели на предварительно разделенных данных\"\"\"\n",
    "        # Обучаем кодировщики на полном наборе данных\n",
    "        all_data = pd.concat([train_data, test_data])\n",
    "        self.user_encoder.fit(all_data[USER_ID])\n",
    "        self.item_encoder.fit(all_data[ITEM_ID])\n",
    "        \n",
    "        # Готовим матрицы\n",
    "        train_matrix = self._prepare_matrix(train_data, USER_ID, ITEM_ID)\n",
    "        test_matrix = self._prepare_matrix(test_data, USER_ID, ITEM_ID)\n",
    "\n",
    "        # Обучаем модель\n",
    "        self.model = AlternatingLeastSquares(\n",
    "            factors=self.factors,\n",
    "            iterations=self.iterations,\n",
    "            regularization=self.regularization,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        self.model.fit(self.alpha * train_matrix, show_progress=True)\n",
    "\n",
    "        return train_matrix, test_matrix\n",
    "    \n",
    "    def _prepare_matrix(self, data, USER_ID, ITEM_ID):\n",
    "        \"\"\"Внутренний метод для подготовки матрицы из данных\"\"\"\n",
    "        user_ids = self.user_encoder.transform(data[USER_ID])\n",
    "        item_ids = self.item_encoder.transform(data[ITEM_ID])\n",
    "        \n",
    "        matrix = sparse.csr_matrix(\n",
    "            (np.ones(len(data)), (user_ids, item_ids)),\n",
    "            shape=(len(self.user_encoder.classes_), len(self.item_encoder.classes_))\n",
    "        )\n",
    "        \n",
    "        weighted_matrix = bm25_weight(matrix, K1=100, B=0.8)\n",
    "        return weighted_matrix.tocsr() if not sparse.isspmatrix_csr(weighted_matrix) else weighted_matrix\n",
    "    \n",
    "    def recommend(self, user_ids, user_items=None, N=10, filter_already_liked_items=True):\n",
    "        \"\"\"Генерация рекомендаций для списка пользователей\"\"\"\n",
    "        if isinstance(user_ids, str):\n",
    "            user_ids = [user_ids]\n",
    "        \n",
    "        # Кодируем пользователей\n",
    "        user_codes = self.user_encoder.transform(user_ids)\n",
    "        \n",
    "        # Получаем рекомендации\n",
    "        recommendations = []\n",
    "        for user_code in user_codes:\n",
    "            recs = self.model.recommend(\n",
    "                userid=user_code, \n",
    "                user_items=user_items,\n",
    "                N=N, \n",
    "                filter_already_liked_items=filter_already_liked_items,\n",
    "            )\n",
    "            # Декодируем ID товаров\n",
    "            item_ids = self.item_encoder.inverse_transform([r[0] for r in recs])\n",
    "            scores = [r[1] for r in recs]\n",
    "            recommendations.append(list(zip(item_ids, scores)))\n",
    "        \n",
    "        return recommendations if len(recommendations) > 1 else recommendations[0]\n",
    "\n",
    "\n",
    "def train_model(train_data, test_data, USER_ID, ITEM_ID, factors=128, iterations=15, alpha=40):\n",
    "    \"\"\"Полный процесс обучения ALS модели на предварительно разделенных данных\"\"\"\n",
    "    als = ALSRecommender(factors=factors, iterations=iterations, alpha=alpha)\n",
    "    train_matrix, test_matrix = als.train(train_data, test_data, USER_ID, ITEM_ID)\n",
    "    return als, train_matrix, test_matrix\n",
    "\n",
    "\n",
    "def evaluate_model(als, train_matrix, test_matrix, articles_df, USER_ID, ITEM_ID, N=30):\n",
    "    from scipy import sparse\n",
    "    from tqdm import tqdm\n",
    "    import pandas as pd\n",
    "\n",
    "    print(\"\\nГенерация рекомендаций...\")\n",
    "    recommendations = []\n",
    "\n",
    "    train_matrix_csr = train_matrix.tocsr() if not sparse.isspmatrix_csr(train_matrix) else train_matrix\n",
    "\n",
    "    for user_idx in tqdm(range(train_matrix_csr.shape[0])):\n",
    "        user_vector = train_matrix_csr[user_idx]\n",
    "\n",
    "        recs = als.model.recommend(\n",
    "            userid=user_idx,\n",
    "            user_items=user_vector,\n",
    "            N=N,\n",
    "            filter_already_liked_items=False\n",
    "        )\n",
    "\n",
    "        item_ids = als.item_encoder.inverse_transform(recs[0])\n",
    "        scores = recs[1]\n",
    "        user_id = als.user_encoder.inverse_transform([user_idx])[0]\n",
    "\n",
    "        recs_df = pd.DataFrame({\n",
    "            USER_ID: [user_id] * len(item_ids),\n",
    "            ITEM_ID: item_ids,\n",
    "            'score': scores\n",
    "        })\n",
    "\n",
    "        recommendations.append(recs_df)\n",
    "\n",
    "    recommendations_df = pd.concat(recommendations, ignore_index=True)\n",
    "\n",
    "    # Конвертируем test_matrix в DataFrame\n",
    "    test_transactions_df = matrix_to_transactions_df(test_matrix, als.user_encoder, als.item_encoder, USER_ID, ITEM_ID)\n",
    "\n",
    "    print(\"\\nОценка качества рекомендаций:\")\n",
    "    metrics = RecommendationMetricsDf(\n",
    "        recommendations_df,\n",
    "        test_transactions_df,\n",
    "        USER_ID,\n",
    "        ITEM_ID\n",
    "    )\n",
    "\n",
    "    print(f\"Precision@{N}: {metrics.precision_at_k(k=N):.4f}\")\n",
    "    print(f\"Recall@{N}: {metrics.recall_at_k(k=N):.4f}\")\n",
    "    print(f\"MAP@{N}: {metrics.map_at_k(k=N):.4f}\")\n",
    "    print(f\"NDCG@{N}: {metrics.ndcg_at_k(k=N):.4f}\")\n",
    "    print(f\"Coverage: {metrics.coverage(len(articles_df)):.2%}\")\n",
    "\n",
    "    return recommendations_df\n",
    "\n",
    "\n",
    "def matrix_to_transactions_df(matrix, user_encoder, item_encoder, USER_ID, ITEM_ID):\n",
    "    \"\"\"Конвертация разреженной матрицы взаимодействий в DataFrame с user_id и item_id\"\"\"\n",
    "    rows, cols = matrix.nonzero()\n",
    "\n",
    "    user_ids = user_encoder.inverse_transform(rows)\n",
    "    item_ids = item_encoder.inverse_transform(cols)\n",
    "\n",
    "    transactions_df = pd.DataFrame({\n",
    "        USER_ID: user_ids,\n",
    "        ITEM_ID: item_ids\n",
    "    })\n",
    "\n",
    "    return transactions_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4ea0ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/implicit/cpu/als.py:95: RuntimeWarning: OpenBLAS is configured to use 12 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
      "  check_blas_config()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03401ec85d0f407f9763dac45b2fdd02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Генерация рекомендаций...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 960.40it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Оценка качества рекомендаций:\n",
      "Precision@10: 0.0306\n",
      "Recall@10: 0.2930\n",
      "MAP@10: 0.1876\n",
      "NDCG@10: 0.2200\n",
      "Coverage: 18.56%\n"
     ]
    }
   ],
   "source": [
    "# Определяем константы для колонок\n",
    "USER_ID = \"user_id\"\n",
    "ITEM_ID = \"item_id\"\n",
    "\n",
    "# 1. Сначала нужно разделить данные на train и test\n",
    "train_data, test_data = time_based_split(filtered_transactions)\n",
    "\n",
    "# 2. Затем обучить модель\n",
    "als, train_matrix, test_matrix = train_model(\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    USER_ID=USER_ID,\n",
    "    ITEM_ID=ITEM_ID,\n",
    "    factors=32,\n",
    "    iterations=30,\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "# 3. Оценить модель\n",
    "all_recommendations = evaluate_model(\n",
    "    als, \n",
    "    train_matrix, \n",
    "    test_matrix, \n",
    "    filtered_articles, \n",
    "    USER_ID, \n",
    "    ITEM_ID, \n",
    "    N=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf46b90",
   "metadata": {},
   "source": [
    "## Bert4Rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdd257d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [00:01<00:00,  7.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 5.3699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [00:01<00:00,  6.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 5.2604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [00:01<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 5.0185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [00:01<00:00,  7.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 4.8731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [00:01<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 4.8463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [00:01<00:00,  7.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 4.5531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [00:01<00:00,  7.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 4.6413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [00:01<00:00,  7.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 4.4465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [00:01<00:00,  7.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 4.2934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [00:01<00:00,  7.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 4.3296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [00:01<00:00,  7.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 4.1568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [00:01<00:00,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 4.3108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [00:01<00:00,  6.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 4.0444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [00:01<00:00,  7.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 3.9847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TransactionDataset(Dataset):\n",
    "    def __init__(self, df, max_len=10):\n",
    "        self.df = df.sort_values(['user_id', 'time']).copy()\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Инициализация энкодеров\n",
    "        self.user_encoder = LabelEncoder()\n",
    "        self.item_encoder = LabelEncoder()\n",
    "        \n",
    "        self.user_encoder.fit(self.df['user_id'].unique())\n",
    "        self.item_encoder.fit(self.df['item_id'].unique())\n",
    "        \n",
    "        # Преобразуем ID в числовые индексы (+1 для PAD)\n",
    "        self.df['user_idx'] = self.user_encoder.transform(self.df['user_id'])\n",
    "        self.df['item_idx'] = self.item_encoder.transform(self.df['item_id']) + 1\n",
    "        \n",
    "        # Группируем транзакции по пользователям\n",
    "        self.user_sequences = self.df.groupby('user_idx')['item_idx'].apply(list).to_dict()\n",
    "        \n",
    "        # Специальные токены\n",
    "        self.PAD = 0\n",
    "        self.MASK = len(self.item_encoder.classes_) + 1\n",
    "        self.CLS = len(self.item_encoder.classes_) + 2\n",
    "        self.SEP = len(self.item_encoder.classes_) + 3\n",
    "        \n",
    "        self.vocab_size = len(self.item_encoder.classes_) + 4\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.user_sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.user_sequences.get(idx, [])\n",
    "        \n",
    "        # Обрезаем последовательность если слишком длинная\n",
    "        if len(seq) > self.max_len - 2:\n",
    "            seq = seq[-(self.max_len - 2):]\n",
    "        \n",
    "        # Добавляем специальные токены\n",
    "        seq = [self.CLS] + seq + [self.SEP]\n",
    "        padding_len = self.max_len - len(seq)\n",
    "        seq = seq + [self.PAD] * padding_len\n",
    "        \n",
    "        # Создаем маскированные данные\n",
    "        masked_seq = seq.copy()\n",
    "        labels = [self.PAD] * len(seq)\n",
    "        \n",
    "        for i in range(1, len(seq)-1):\n",
    "            if seq[i] == self.PAD:\n",
    "                continue\n",
    "            if np.random.random() < 0.15:\n",
    "                labels[i] = seq[i]\n",
    "                p = np.random.random()\n",
    "                if p < 0.8:\n",
    "                    masked_seq[i] = self.MASK\n",
    "                elif p < 0.9:\n",
    "                    masked_seq[i] = np.random.randint(1, len(self.item_encoder.classes_)+1)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(masked_seq, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor([1 if x != self.PAD else 0 for x in seq], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=10):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class BERT4Rec(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=64, nhead=2, num_layers=2, max_len=10):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.item_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model*4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, src, src_key_padding_mask=None):\n",
    "        src = self.item_emb(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        \n",
    "        output = self.transformer(\n",
    "            src, \n",
    "            src_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "        \n",
    "        return self.output(output)\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(input_ids, src_key_padding_mask=(attention_mask == 0))\n",
    "        \n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, model.vocab_size),\n",
    "            labels.view(-1),\n",
    "            ignore_index=0\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * len(input_ids)\n",
    "        total_samples += len(input_ids)\n",
    "    \n",
    "    return total_loss / total_samples if total_samples > 0 else 0\n",
    "\n",
    "def predict_next_item(model, dataset, user_id, device, top_k=5):\n",
    "    model.eval()\n",
    "    \n",
    "    user_idx = dataset.user_encoder.transform([user_id])[0]\n",
    "    seq = dataset.user_sequences[user_idx]\n",
    "    \n",
    "    if len(seq) > dataset.max_len - 2:\n",
    "        seq = seq[-(dataset.max_len - 2):]\n",
    "    \n",
    "    # Create the input sequence with CLS and SEP tokens\n",
    "    input_seq = [dataset.CLS] + seq + [dataset.SEP]\n",
    "    padding_len = dataset.max_len - len(input_seq)\n",
    "    input_seq = input_seq + [dataset.PAD] * padding_len\n",
    "    \n",
    "    # Create attention mask\n",
    "    attention_mask = torch.tensor([[1 if x != dataset.PAD else 0 for x in input_seq]], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Replace the last non-padding item with MASK\n",
    "    masked_seq = input_seq.copy()\n",
    "    if len(seq) > 0:  # Only if there are items to predict\n",
    "        last_item_pos = len([dataset.CLS] + seq)  # Position of the item before SEP\n",
    "        masked_seq[last_item_pos] = dataset.MASK\n",
    "    \n",
    "    input_ids = torch.tensor([masked_seq], dtype=torch.long).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, src_key_padding_mask=(attention_mask == 0))\n",
    "        \n",
    "        # Get the prediction for the masked position\n",
    "        if len(seq) > 0:\n",
    "            pred_pos = last_item_pos\n",
    "        else:\n",
    "            pred_pos = 1  # If sequence is empty, predict after CLS\n",
    "        \n",
    "        probs = F.softmax(logits[0, pred_pos], dim=-1)  # Prediction for the masked position\n",
    "        \n",
    "        # Exclude special tokens (PAD=0, MASK, CLS, SEP)\n",
    "        valid_items = torch.arange(1, len(dataset.item_encoder.classes_)+1).to(device)\n",
    "        valid_probs = probs[valid_items]\n",
    "        \n",
    "        # Ensure we don't request more items than available\n",
    "        actual_top_k = min(top_k, len(valid_items))\n",
    "        if actual_top_k <= 0:\n",
    "            return np.array([]), np.array([])\n",
    "            \n",
    "        top_probs, top_indices = torch.topk(valid_probs, actual_top_k)\n",
    "        top_items = valid_items[top_indices].cpu().numpy()\n",
    "        \n",
    "        return top_items, top_probs.cpu().numpy()\n",
    "\n",
    "\n",
    "# Создаем датасет\n",
    "max_len = 128\n",
    "bert_dataset = TransactionDataset(filtered_transactions, max_len=max_len)\n",
    "bert_train_loader = DataLoader(bert_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Инициализируем модель\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bert4rec_model = BERT4Rec(\n",
    "    vocab_size=bert_dataset.vocab_size,\n",
    "    d_model=128,\n",
    "    nhead=2,\n",
    "    num_layers=2,\n",
    "    max_len=max_len,\n",
    ").to(device)\n",
    "bert_optimizer = torch.optim.Adam(bert4rec_model.parameters(), lr=0.001)\n",
    "\n",
    "# Обучение\n",
    "for epoch in range(1, 15):\n",
    "    loss = train_epoch(bert4rec_model, bert_train_loader, bert_optimizer, device)\n",
    "    print(f'Epoch {epoch}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "add4e3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating recommendations:   0%|          | 0/500 [00:00<?, ?it/s]/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n",
      "Generating recommendations: 100%|██████████| 500/500 [00:00<00:00, 1006.64it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_all_recommendations(model, dataset, device, top_k=10):\n",
    "    \"\"\"Генерирует рекомендации для всех пользователей\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    user_ids = dataset.user_encoder.classes_\n",
    "    item_decoder = {idx+1: item_id for idx, item_id in enumerate(dataset.item_encoder.classes_)}  # item_idx -> item_id\n",
    "\n",
    "    recommendations = []\n",
    "\n",
    "    for user_id in tqdm(user_ids, desc=\"Generating recommendations\"):\n",
    "        top_items, top_probs = predict_next_item(model, dataset, user_id, device, top_k=top_k)\n",
    "\n",
    "        for item_idx, score in zip(top_items, top_probs):\n",
    "            if item_idx in item_decoder:\n",
    "                item_id = item_decoder[item_idx]\n",
    "                recommendations.append({\n",
    "                    'user_id': user_id,\n",
    "                    'item_id': item_id,\n",
    "                    'score': score\n",
    "                })\n",
    "\n",
    "    rec_df = pd.DataFrame(recommendations)\n",
    "    return rec_df\n",
    "\n",
    "\n",
    "bert4rec_all_recommendations = get_all_recommendations(\n",
    "    model=bert4rec_model,\n",
    "    dataset=bert_dataset,\n",
    "    device=device,\n",
    "    top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcd7e656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Оценка качества рекомендаций:\n",
      "Precision@10: 0.0330\n",
      "Recall@10: 0.3130\n",
      "MAP@10: 0.1804\n",
      "NDCG@10: 0.2210\n",
      "Coverage: 6.56%\n"
     ]
    }
   ],
   "source": [
    "# Расчет метрик\n",
    "N = 10\n",
    "test_transactions_df = matrix_to_transactions_df(test_matrix, als.user_encoder, als.item_encoder, USER_ID, ITEM_ID)\n",
    "print(\"\\nОценка качества рекомендаций:\")\n",
    "metrics = RecommendationMetricsDf(\n",
    "    bert4rec_all_recommendations,\n",
    "    test_transactions_df,\n",
    "    USER_ID,\n",
    "    ITEM_ID\n",
    ")\n",
    "\n",
    "print(f\"Precision@{N}: {metrics.precision_at_k(k=N):.4f}\")\n",
    "print(f\"Recall@{N}: {metrics.recall_at_k(k=N):.4f}\")\n",
    "print(f\"MAP@{N}: {metrics.map_at_k(k=N):.4f}\")\n",
    "print(f\"NDCG@{N}: {metrics.ndcg_at_k(k=N):.4f}\")\n",
    "print(f\"Coverage: {metrics.coverage(len(filtered_articles)):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccabda82",
   "metadata": {},
   "source": [
    "## Graph Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fe19710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dn/wx1p5g8j6mz37qdp8g5wf91w0000gn/T/ipykernel_12156/1200430458.py:92: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  edge_index = torch.tensor([\n",
      " 40%|████      | 12/30 [00:00<00:00, 116.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 80.1835\n",
      "Epoch 1: Loss = 40.7910\n",
      "Epoch 2: Loss = 21.2131\n",
      "Epoch 3: Loss = 16.3981\n",
      "Epoch 4: Loss = 12.7614\n",
      "Epoch 5: Loss = 14.5612\n",
      "Epoch 6: Loss = 12.8535\n",
      "Epoch 7: Loss = 11.5523\n",
      "Epoch 8: Loss = 11.0745\n",
      "Epoch 9: Loss = 13.7444\n",
      "Epoch 10: Loss = 10.2415\n",
      "Epoch 11: Loss = 8.9759\n",
      "Epoch 12: Loss = 10.5023\n",
      "Epoch 13: Loss = 8.3523\n",
      "Epoch 14: Loss = 8.8585\n",
      "Epoch 15: Loss = 8.0248\n",
      "Epoch 16: Loss = 8.8913\n",
      "Epoch 17: Loss = 6.4208\n",
      "Epoch 18: Loss = 6.9595\n",
      "Epoch 19: Loss = 6.9556\n",
      "Epoch 20: Loss = 6.5580\n",
      "Epoch 21: Loss = 7.2913\n",
      "Epoch 22: Loss = 5.4166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 107.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Loss = 6.0731\n",
      "Epoch 24: Loss = 6.0455\n",
      "Epoch 25: Loss = 5.2909\n",
      "Epoch 26: Loss = 6.1634\n",
      "Epoch 27: Loss = 4.9371\n",
      "Epoch 28: Loss = 4.4870\n",
      "Epoch 29: Loss = 5.2452\n",
      "Precision@10: 0.0360\n",
      "Recall@10: 0.3480\n",
      "MAP@10: 0.2394\n",
      "NDCG@10: 0.2705\n",
      "Coverage: 39.11%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class GCNRecommender(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        \n",
    "        self.user_embed = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embed = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        self.conv1 = GCNConv(embedding_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, edge_index):\n",
    "        user_emb = self.user_embed(torch.arange(self.num_users, device=edge_index.device))\n",
    "        item_emb = self.item_embed(torch.arange(self.num_items, device=edge_index.device))\n",
    "        x = torch.cat([user_emb, item_emb], dim=0)\n",
    "        \n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        user_final = x[:self.num_users]\n",
    "        item_final = x[self.num_users:]\n",
    "        \n",
    "        return torch.sigmoid(torch.mm(user_final, item_final.t()))\n",
    "\n",
    "def prepare_data(data: pd.DataFrame, \n",
    "                user_col: str = 'user_id',\n",
    "                time_col: str = 'time',\n",
    "                test_ratio: float = 0.2) -> tuple:\n",
    "    \"\"\"Подготовка данных с временным разделением для графовых сетей\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame с колонками user_id, item_id, time\n",
    "        user_col: название колонки с идентификаторами пользователей\n",
    "        time_col: название колонки с временными метками\n",
    "        test_ratio: доля тестовых данных\n",
    "    \n",
    "    Returns:\n",
    "        Кортеж (train_data, test_data, user_encoder, item_encoder)\n",
    "    \"\"\"\n",
    "    # Сортируем по пользователю и времени\n",
    "    data = data.sort_values([user_col, time_col])\n",
    "    \n",
    "    # Функция для разделения данных пользователя\n",
    "    def split_user_group(df):\n",
    "        split_idx = int(len(df) * (1 - test_ratio))\n",
    "        train = df.iloc[:split_idx]\n",
    "        test = df.iloc[split_idx:]\n",
    "        return train, test\n",
    "    \n",
    "    # Разделяем данные для каждого пользователя\n",
    "    grouped = data.groupby(user_col, group_keys=False)\n",
    "    train_data = grouped.apply(lambda x: split_user_group(x)[0])\n",
    "    test_data = grouped.apply(lambda x: split_user_group(x)[1])\n",
    "    \n",
    "    # Кодируем пользователей и товары (только на train)\n",
    "    user_encoder = LabelEncoder()\n",
    "    item_encoder = LabelEncoder()\n",
    "    \n",
    "    # Фитируем кодировщики на train данных\n",
    "    train_data['user_idx'] = user_encoder.fit_transform(train_data[user_col])\n",
    "    train_data['item_idx'] = item_encoder.fit_transform(train_data['item_id'])\n",
    "    \n",
    "    # Фильтруем тестовые данные (только известные пользователи и товары)\n",
    "    test_data = test_data[test_data[user_col].isin(user_encoder.classes_)]\n",
    "    test_data = test_data[test_data['item_id'].isin(item_encoder.classes_)]\n",
    "    \n",
    "    # Кодируем тестовые данные\n",
    "    test_data['user_idx'] = user_encoder.transform(test_data[user_col])\n",
    "    test_data['item_idx'] = item_encoder.transform(test_data['item_id'])\n",
    "    \n",
    "    # Проверяем, что в тестовых данных есть хотя бы одно взаимодействие\n",
    "    if len(test_data) == 0:\n",
    "        raise ValueError(\"Test data is empty after filtering. Check your data split.\")\n",
    "    \n",
    "    return train_data, test_data, user_encoder, item_encoder\n",
    "\n",
    "def train_model(train_data, num_users, num_items, epochs=50):\n",
    "    # Создание графа\n",
    "    edge_index = torch.tensor([\n",
    "        train_data['user_idx'].values,\n",
    "        train_data['item_idx'].values + num_users\n",
    "    ], dtype=torch.long)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = GCNRecommender(num_users, num_items).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCELoss()\n",
    "    edge_index = edge_index.to(device)\n",
    "    \n",
    "    # Негативное сэмплирование\n",
    "    def create_batch(data, batch_size=1024, neg_ratio=1):\n",
    "        users = data['user_idx'].values\n",
    "        items = data['item_idx'].values\n",
    "        \n",
    "        neg_users = np.repeat(users, neg_ratio)\n",
    "        neg_items = np.random.choice(num_items, len(users)*neg_ratio)\n",
    "        \n",
    "        all_users = np.concatenate([users, neg_users])\n",
    "        all_items = np.concatenate([items, neg_items])\n",
    "        labels = np.concatenate([\n",
    "            np.ones(len(users)),\n",
    "            np.zeros(len(users)*neg_ratio)\n",
    "        ])\n",
    "        \n",
    "        indices = np.random.permutation(len(all_users))\n",
    "        for i in range(0, len(indices), batch_size):\n",
    "            batch_idx = indices[i:i+batch_size]\n",
    "            yield (\n",
    "                torch.LongTensor(all_users[batch_idx]).to(device),\n",
    "                torch.LongTensor(all_items[batch_idx]).to(device),\n",
    "                torch.FloatTensor(labels[batch_idx]).to(device)\n",
    "            )\n",
    "    \n",
    "    # Обучение\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for users, items, labels in create_batch(train_data):\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(edge_index)[users, items]\n",
    "            loss = criterion(preds, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch}: Loss = {total_loss:.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def generate_recommendations(model, user_encoder, item_encoder, k=10):\n",
    "    model.eval()\n",
    "    num_users = len(user_encoder.classes_)\n",
    "    num_items = len(item_encoder.classes_)\n",
    "    \n",
    "    # Используем оригинальный edge_index из обучения\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        preds = model(model.edge_index).cpu().numpy()\n",
    "    \n",
    "    recommendations = []\n",
    "    for user_idx in range(num_users):\n",
    "        user_id = user_encoder.inverse_transform([user_idx])[0]\n",
    "        scores = preds[user_idx]\n",
    "        top_items = np.argsort(-scores)[:k]\n",
    "        \n",
    "        for item_idx in top_items:\n",
    "            item_id = item_encoder.inverse_transform([item_idx])[0]\n",
    "            recommendations.append({\n",
    "                'user_id': user_id,\n",
    "                'item_id': item_id,\n",
    "                'score': scores[item_idx]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(recommendations)\n",
    "\n",
    "# Полный пайплайн\n",
    "if __name__ == \"__main__\":\n",
    "    train_data, test_data, user_encoder, item_encoder = prepare_data(filtered_transactions)\n",
    "    \n",
    "    num_users = len(user_encoder.classes_)\n",
    "    num_items = len(item_encoder.classes_)\n",
    "    \n",
    "    model = train_model(train_data, num_users, num_items, epochs=30)\n",
    "    model.edge_index = torch.tensor([\n",
    "        train_data['user_idx'].values,\n",
    "        train_data['item_idx'].values + num_users\n",
    "    ], dtype=torch.long).to(next(model.parameters()).device)\n",
    "    \n",
    "    recommendations = generate_recommendations(model, user_encoder, item_encoder)\n",
    "    \n",
    "    # Расчет метрик\n",
    "    metrics = RecommendationMetricsDf(recommendations, test_data, USER_ID, ITEM_ID)\n",
    "    print(f\"Precision@{N}: {metrics.precision_at_k(k=N):.4f}\")\n",
    "    print(f\"Recall@{N}: {metrics.recall_at_k(k=N):.4f}\")\n",
    "    print(f\"MAP@{N}: {metrics.map_at_k(k=N):.4f}\")\n",
    "    print(f\"NDCG@{N}: {metrics.ndcg_at_k(k=N):.4f}\")\n",
    "    print(f\"Coverage: {metrics.coverage(len(filtered_articles)):.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d33c436",
   "metadata": {},
   "source": [
    "## GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f5a78fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.0034\n",
      "Epoch 1: Loss = 0.0023\n",
      "Epoch 2: Loss = 0.0023\n",
      "Epoch 3: Loss = 0.0021\n",
      "Epoch 4: Loss = 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [00:00<00:00, 108.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss = 0.0019\n",
      "Epoch 6: Loss = 0.0018\n",
      "Epoch 7: Loss = 0.0018\n",
      "Epoch 8: Loss = 0.0017\n",
      "Epoch 9: Loss = 0.0016\n",
      "Epoch 10: Loss = 0.0015\n",
      "Epoch 11: Loss = 0.0015\n",
      "Epoch 12: Loss = 0.0015\n",
      "Epoch 13: Loss = 0.0014\n",
      "Epoch 14: Loss = 0.0014\n",
      "Epoch 15: Loss = 0.0013\n",
      "Epoch 16: Loss = 0.0013\n",
      "Epoch 17: Loss = 0.0013\n",
      "Epoch 18: Loss = 0.0012\n",
      "Epoch 19: Loss = 0.0012\n",
      "Epoch 20: Loss = 0.0013\n",
      "Epoch 21: Loss = 0.0012\n",
      "Epoch 22: Loss = 0.0012\n",
      "Epoch 23: Loss = 0.0012\n",
      "Epoch 24: Loss = 0.0011\n",
      "Epoch 25: Loss = 0.0012\n",
      "Epoch 26: Loss = 0.0011\n",
      "Epoch 27: Loss = 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 102.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Loss = 0.0011\n",
      "Epoch 29: Loss = 0.0011\n",
      "Precision@10: 0.0246\n",
      "Recall@10: 0.2400\n",
      "MAP@10: 0.1455\n",
      "NDCG@10: 0.1721\n",
      "Coverage: 37.67%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "class GraphSAGERecommender(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        \n",
    "        self.user_embed = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embed = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        # Заменяем GCNConv на SAGEConv\n",
    "        self.conv1 = SAGEConv(embedding_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, edge_index):\n",
    "        user_emb = self.user_embed(torch.arange(self.num_users, device=edge_index.device))\n",
    "        item_emb = self.item_embed(torch.arange(self.num_items, device=edge_index.device))\n",
    "        x = torch.cat([user_emb, item_emb], dim=0)\n",
    "        \n",
    "        # Графовые свертки SAGE\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        user_final = x[:self.num_users]\n",
    "        item_final = x[self.num_users:]\n",
    "        \n",
    "        return torch.sigmoid(torch.mm(user_final, item_final.t()))\n",
    "\n",
    "def prepare_data(data: pd.DataFrame, \n",
    "                user_col: str = 'user_id',\n",
    "                time_col: str = 'time',\n",
    "                test_ratio: float = 0.2) -> tuple:\n",
    "    \"\"\"Подготовка данных с временным разделением для графовых сетей\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame с колонками user_id, item_id, time\n",
    "        user_col: название колонки с идентификаторами пользователей\n",
    "        time_col: название колонки с временными метками\n",
    "        test_ratio: доля тестовых данных\n",
    "    \n",
    "    Returns:\n",
    "        Кортеж (train_data, test_data, user_encoder, item_encoder)\n",
    "    \"\"\"\n",
    "    # Сортируем по пользователю и времени\n",
    "    data = data.sort_values([user_col, time_col])\n",
    "    \n",
    "    # Функция для разделения данных пользователя\n",
    "    def split_user_group(df):\n",
    "        split_idx = int(len(df) * (1 - test_ratio))\n",
    "        train = df.iloc[:split_idx]\n",
    "        test = df.iloc[split_idx:]\n",
    "        return train, test\n",
    "    \n",
    "    # Разделяем данные для каждого пользователя\n",
    "    grouped = data.groupby(user_col, group_keys=False)\n",
    "    train_data = grouped.apply(lambda x: split_user_group(x)[0])\n",
    "    test_data = grouped.apply(lambda x: split_user_group(x)[1])\n",
    "    \n",
    "    # Кодируем пользователей и товары (только на train)\n",
    "    user_encoder = LabelEncoder()\n",
    "    item_encoder = LabelEncoder()\n",
    "    \n",
    "    # Фитируем кодировщики на train данных\n",
    "    train_data['user_idx'] = user_encoder.fit_transform(train_data[user_col])\n",
    "    train_data['item_idx'] = item_encoder.fit_transform(train_data['item_id'])\n",
    "    \n",
    "    # Фильтруем тестовые данные (только известные пользователи и товары)\n",
    "    test_data = test_data[test_data[user_col].isin(user_encoder.classes_)]\n",
    "    test_data = test_data[test_data['item_id'].isin(item_encoder.classes_)]\n",
    "    \n",
    "    # Кодируем тестовые данные\n",
    "    test_data['user_idx'] = user_encoder.transform(test_data[user_col])\n",
    "    test_data['item_idx'] = item_encoder.transform(test_data['item_id'])\n",
    "    \n",
    "    # Проверяем, что в тестовых данных есть хотя бы одно взаимодействие\n",
    "    if len(test_data) == 0:\n",
    "        raise ValueError(\"Test data is empty after filtering. Check your data split.\")\n",
    "    \n",
    "    return train_data, test_data, user_encoder, item_encoder\n",
    "\n",
    "def train_model(train_data, num_users, num_items, epochs=50):\n",
    "    \"\"\"Модифицируем для GraphSAGE\"\"\"\n",
    "    edge_index = torch.tensor([\n",
    "        train_data['user_idx'].values,\n",
    "        train_data['item_idx'].values + num_users\n",
    "    ], dtype=torch.long)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = GraphSAGERecommender(num_users, num_items).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # Добавил регуляризацию\n",
    "    criterion = nn.BCELoss()\n",
    "    edge_index = edge_index.to(device)\n",
    "    \n",
    "    # Негативное сэмплирование (без изменений)\n",
    "    def create_batch(data, batch_size=1024, neg_ratio=1):\n",
    "        users = data['user_idx'].values\n",
    "        items = data['item_idx'].values\n",
    "        \n",
    "        neg_users = np.repeat(users, neg_ratio)\n",
    "        neg_items = np.random.choice(num_items, len(users)*neg_ratio)\n",
    "        \n",
    "        all_users = np.concatenate([users, neg_users])\n",
    "        all_items = np.concatenate([items, neg_items])\n",
    "        labels = np.concatenate([\n",
    "            np.ones(len(users)),\n",
    "            np.zeros(len(users)*neg_ratio)\n",
    "        ])\n",
    "        \n",
    "        indices = np.random.permutation(len(all_users))\n",
    "        for i in range(0, len(indices), batch_size):\n",
    "            batch_idx = indices[i:i+batch_size]\n",
    "            yield (\n",
    "                torch.LongTensor(all_users[batch_idx]).to(device),\n",
    "                torch.LongTensor(all_items[batch_idx]).to(device),\n",
    "                torch.FloatTensor(labels[batch_idx]).to(device)\n",
    "            )\n",
    "    \n",
    "    # Обучение с ранней остановкой\n",
    "    best_loss = float('inf')\n",
    "    patience = 5\n",
    "    no_improve = 0\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for users, items, labels in create_batch(train_data):\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(edge_index)[users, items]\n",
    "            loss = criterion(preds, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_data)\n",
    "        print(f'Epoch {epoch}: Loss = {avg_loss:.4f}')\n",
    "        \n",
    "        # Ранняя остановка\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            no_improve = 0\n",
    "            torch.save(model.state_dict(), 'best_graphsage_model.pth')\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "    \n",
    "    model.load_state_dict(torch.load('best_graphsage_model.pth'))\n",
    "    return model\n",
    "\n",
    "def generate_recommendations(model, user_encoder, item_encoder, k=10):\n",
    "    \"\"\"Аналогично вашему коду, без изменений\"\"\"\n",
    "    model.eval()\n",
    "    num_users = len(user_encoder.classes_)\n",
    "    num_items = len(item_encoder.classes_)\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        preds = model(model.edge_index).cpu().numpy()\n",
    "    \n",
    "    recommendations = []\n",
    "    for user_idx in range(num_users):\n",
    "        user_id = user_encoder.inverse_transform([user_idx])[0]\n",
    "        scores = preds[user_idx]\n",
    "        top_items = np.argsort(-scores)[:k]\n",
    "        \n",
    "        for item_idx in top_items:\n",
    "            item_id = item_encoder.inverse_transform([item_idx])[0]\n",
    "            recommendations.append({\n",
    "                'user_id': user_id,\n",
    "                'item_id': item_id,\n",
    "                'score': scores[item_idx]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(recommendations)\n",
    "\n",
    "# Полный пайплайн\n",
    "if __name__ == \"__main__\":\n",
    "    # Подготовка данных\n",
    "    train_data, test_data, user_encoder, item_encoder = prepare_data(filtered_transactions)\n",
    "    \n",
    "    num_users = len(user_encoder.classes_)\n",
    "    num_items = len(item_encoder.classes_)\n",
    "    \n",
    "    # Обучение GraphSAGE\n",
    "    model = train_model(train_data, num_users, num_items, epochs=30)\n",
    "    model.edge_index = torch.tensor([\n",
    "        train_data['user_idx'].values,\n",
    "        train_data['item_idx'].values + num_users\n",
    "    ], dtype=torch.long).to(next(model.parameters()).device)\n",
    "    \n",
    "    # Генерация рекомендаций\n",
    "    recommendations = generate_recommendations(model, user_encoder, item_encoder)\n",
    "    \n",
    "    # Расчет метрик\n",
    "    metrics = RecommendationMetricsDf(recommendations, test_data, USER_ID, ITEM_ID)\n",
    "    print(f\"Precision@{N}: {metrics.precision_at_k(k=N):.4f}\")\n",
    "    print(f\"Recall@{N}: {metrics.recall_at_k(k=N):.4f}\")\n",
    "    print(f\"MAP@{N}: {metrics.map_at_k(k=N):.4f}\")\n",
    "    print(f\"NDCG@{N}: {metrics.ndcg_at_k(k=N):.4f}\")\n",
    "    print(f\"Coverage: {metrics.coverage(len(filtered_articles)):.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ce0af1",
   "metadata": {},
   "source": [
    "## LightGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94bf65cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:00<00:22,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 2.0117, Test AUC = 0.7409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:00<00:06, 13.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss = 1.9883, Test AUC = 0.7485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:00<00:05, 17.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss = 1.9587, Test AUC = 0.7561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [00:00<00:04, 18.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Loss = 1.9223, Test AUC = 0.7618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [00:01<00:04, 19.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Loss = 1.8849, Test AUC = 0.7645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [00:01<00:03, 19.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Loss = 1.8475, Test AUC = 0.7639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [00:01<00:03, 20.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Loss = 1.8017, Test AUC = 0.7641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [00:01<00:03, 20.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: Loss = 1.7606, Test AUC = 0.7663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [00:02<00:02, 20.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: Loss = 1.7261, Test AUC = 0.7641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [00:02<00:02, 20.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: Loss = 1.6953, Test AUC = 0.7650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [00:02<00:02, 20.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Loss = 1.6469, Test AUC = 0.7634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56/100 [00:02<00:02, 20.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: Loss = 1.6142, Test AUC = 0.7619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [00:03<00:02, 19.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10: 0.0444\n",
      "Recall@10: 0.4290\n",
      "MAP@10: 0.2613\n",
      "NDCG@10: 0.3087\n",
      "Coverage: 33.78%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from typing import List, Optional\n",
    "\n",
    "class LightGCNConv(MessagePassing):\n",
    "    def __init__(self):\n",
    "        super().__init__(aggr='add')\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.propagate(edge_index, x=x)\n",
    "\n",
    "    def message_and_aggregate(self, x_j):\n",
    "        return x_j\n",
    "\n",
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, num_users: int, num_items: int, embedding_dim: int = 64, \n",
    "                 n_layers: int = 3, keep_prob: float = 0.6):\n",
    "        super().__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.keep_prob = keep_prob\n",
    "        \n",
    "        # Инициализация эмбеддингов\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        # Слои LightGCN\n",
    "        self.convs = nn.ModuleList([LightGCNConv() for _ in range(n_layers)])\n",
    "        \n",
    "        # Инициализация весов\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.user_embedding.weight, std=0.1)\n",
    "        nn.init.normal_(self.item_embedding.weight, std=0.1)\n",
    "\n",
    "    def forward(self, edge_index: torch.Tensor):\n",
    "        # Получаем эмбеддинги пользователей и товаров\n",
    "        user_emb = self.user_embedding.weight\n",
    "        item_emb = self.item_embedding.weight\n",
    "        x = torch.cat([user_emb, item_emb], dim=0)\n",
    "        \n",
    "        # Создаем список эмбеддингов на каждом слое\n",
    "        embeddings = [x]\n",
    "        \n",
    "        # Пропагация через слои LightGCN\n",
    "        for i in range(self.n_layers):\n",
    "            # Dropout для ребер\n",
    "            if self.training and self.keep_prob < 1:\n",
    "                edge_index = self.random_drop_edges(edge_index)\n",
    "            \n",
    "            x = self.convs[i](x, edge_index)\n",
    "            embeddings.append(x)\n",
    "        \n",
    "        # Комбинируем эмбеддинги со всех слоев\n",
    "        final_embeddings = torch.mean(torch.stack(embeddings, dim=0), dim=0)\n",
    "        \n",
    "        # Разделяем пользователей и товары\n",
    "        user_final, item_final = torch.split(\n",
    "            final_embeddings, [self.num_users, self.num_items]\n",
    "        )\n",
    "        \n",
    "        # Вычисляем предсказания\n",
    "        return torch.sigmoid(torch.mm(user_final, item_final.t()))\n",
    "\n",
    "    def random_drop_edges(self, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        if self.keep_prob >= 1:\n",
    "            return edge_index\n",
    "            \n",
    "        num_edges = edge_index.size(1)\n",
    "        mask = torch.rand(num_edges, device=edge_index.device) < self.keep_prob\n",
    "        return edge_index[:, mask]\n",
    "\n",
    "def prepare_data(data: pd.DataFrame, \n",
    "                user_col: str = 'user_id',\n",
    "                time_col: str = 'time',\n",
    "                test_ratio: float = 0.2) -> tuple:\n",
    "    \"\"\"Подготовка данных с временным разделением для графовых сетей\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame с колонками user_id, item_id, time\n",
    "        user_col: название колонки с идентификаторами пользователей\n",
    "        time_col: название колонки с временными метками\n",
    "        test_ratio: доля тестовых данных\n",
    "    \n",
    "    Returns:\n",
    "        Кортеж (train_data, test_data, user_encoder, item_encoder)\n",
    "    \"\"\"\n",
    "    # Сортируем по пользователю и времени\n",
    "    data = data.sort_values([user_col, time_col])\n",
    "    \n",
    "    # Функция для разделения данных пользователя\n",
    "    def split_user_group(df):\n",
    "        split_idx = int(len(df) * (1 - test_ratio))\n",
    "        train = df.iloc[:split_idx]\n",
    "        test = df.iloc[split_idx:]\n",
    "        return train, test\n",
    "    \n",
    "    # Разделяем данные для каждого пользователя\n",
    "    grouped = data.groupby(user_col, group_keys=False)\n",
    "    train_data = grouped.apply(lambda x: split_user_group(x)[0])\n",
    "    test_data = grouped.apply(lambda x: split_user_group(x)[1])\n",
    "    \n",
    "    # Кодируем пользователей и товары (только на train)\n",
    "    user_encoder = LabelEncoder()\n",
    "    item_encoder = LabelEncoder()\n",
    "    \n",
    "    # Фитируем кодировщики на train данных\n",
    "    train_data['user_idx'] = user_encoder.fit_transform(train_data[user_col])\n",
    "    train_data['item_idx'] = item_encoder.fit_transform(train_data['item_id'])\n",
    "    \n",
    "    # Фильтруем тестовые данные (только известные пользователи и товары)\n",
    "    test_data = test_data[test_data[user_col].isin(user_encoder.classes_)]\n",
    "    test_data = test_data[test_data['item_id'].isin(item_encoder.classes_)]\n",
    "    \n",
    "    # Кодируем тестовые данные\n",
    "    test_data['user_idx'] = user_encoder.transform(test_data[user_col])\n",
    "    test_data['item_idx'] = item_encoder.transform(test_data['item_id'])\n",
    "    \n",
    "    # Проверяем, что в тестовых данных есть хотя бы одно взаимодействие\n",
    "    if len(test_data) == 0:\n",
    "        raise ValueError(\"Test data is empty after filtering. Check your data split.\")\n",
    "    \n",
    "    return train_data, test_data, user_encoder, item_encoder\n",
    "\n",
    "def train_lightgcn(train_data: pd.DataFrame, num_users: int, num_items: int, \n",
    "                 epochs: int = 100, batch_size: int = 1024):\n",
    "    # Создаем граф\n",
    "    edge_index = torch.tensor([\n",
    "        train_data['user_idx'].values,\n",
    "        train_data['item_idx'].values + num_users\n",
    "    ], dtype=torch.long)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = LightGCN(num_users, num_items, embedding_dim=64, n_layers=2, keep_prob=0.8).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCELoss()\n",
    "    edge_index = edge_index.to(device)\n",
    "    \n",
    "    # Негативное сэмплирование\n",
    "    def create_batch(data, batch_size=batch_size, neg_ratio=1):\n",
    "        users = data['user_idx'].values\n",
    "        items = data['item_idx'].values\n",
    "        \n",
    "        neg_users = np.repeat(users, neg_ratio)\n",
    "        neg_items = np.random.choice(num_items, len(users)*neg_ratio)\n",
    "        \n",
    "        all_users = np.concatenate([users, neg_users])\n",
    "        all_items = np.concatenate([items, neg_items])\n",
    "        labels = np.concatenate([\n",
    "            np.ones(len(users)),\n",
    "            np.zeros(len(users)*neg_ratio)\n",
    "        ])\n",
    "        \n",
    "        indices = np.random.permutation(len(all_users))\n",
    "        for i in range(0, len(indices), batch_size):\n",
    "            batch_idx = indices[i:i+batch_size]\n",
    "            yield (\n",
    "                torch.LongTensor(all_users[batch_idx]).to(device),\n",
    "                torch.LongTensor(all_items[batch_idx]).to(device),\n",
    "                torch.FloatTensor(labels[batch_idx]).to(device)\n",
    "            )\n",
    "    \n",
    "    # Обучение с ранней остановкой\n",
    "    best_auc = 0\n",
    "    patience = 5\n",
    "    no_improve = 0\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for users, items, labels in create_batch(train_data):\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(edge_index)[users, items]\n",
    "            loss = criterion(preds, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Оценка\n",
    "        if epoch % 5 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                preds = model(edge_index).cpu()\n",
    "                \n",
    "                test_users = test_data['user_idx'].unique()\n",
    "                if len(test_users) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                auc_scores = []\n",
    "                for user in test_users:\n",
    "                    pos_items = test_data[test_data['user_idx'] == user]['item_idx'].values\n",
    "                    if len(pos_items) == 0:\n",
    "                        continue\n",
    "                        \n",
    "                    neg_items = np.random.choice(num_items, min(99, num_items), replace=False)\n",
    "                    all_items = np.concatenate([pos_items, neg_items])\n",
    "                    labels = np.concatenate([np.ones(len(pos_items)), np.zeros(len(neg_items))])\n",
    "                    \n",
    "                    user_preds = preds[user, all_items].numpy()\n",
    "                    auc_scores.append(roc_auc_score(labels, user_preds))\n",
    "                \n",
    "                current_auc = np.mean(auc_scores) if auc_scores else 0.0\n",
    "                if current_auc > best_auc:\n",
    "                    best_auc = current_auc\n",
    "                    no_improve = 0\n",
    "                    # Сохраняем только параметры модели (без edge_index)\n",
    "                    torch.save({\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'num_users': num_users,\n",
    "                        'num_items': num_items\n",
    "                    }, 'best_lightgcn_model.pth')\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "                    if no_improve >= patience:\n",
    "                        print(\"Early stopping triggered\")\n",
    "                        break\n",
    "                \n",
    "                print(f'Epoch {epoch}: Loss = {total_loss:.4f}, Test AUC = {current_auc:.4f}')\n",
    "    \n",
    "    # Загружаем лучшую модель\n",
    "    checkpoint = torch.load('best_lightgcn_model.pth')\n",
    "    model = LightGCN(\n",
    "        checkpoint['num_users'], \n",
    "        checkpoint['num_items'],\n",
    "        embedding_dim=64, \n",
    "        n_layers=2, \n",
    "        keep_prob=0.8\n",
    "    ).to(device)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.edge_index = edge_index  # Сохраняем edge_index в модели\n",
    "    \n",
    "    return model\n",
    "\n",
    "def generate_recommendations(model, user_encoder, item_encoder, k=10):\n",
    "    \"\"\"Генерация рекомендаций (без изменений)\"\"\"\n",
    "    model.eval()\n",
    "    num_users = len(user_encoder.classes_)\n",
    "    num_items = len(item_encoder.classes_)\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        preds = model(model.edge_index).cpu().numpy()\n",
    "    \n",
    "    recommendations = []\n",
    "    for user_idx in range(num_users):\n",
    "        user_id = user_encoder.inverse_transform([user_idx])[0]\n",
    "        scores = preds[user_idx]\n",
    "        top_items = np.argsort(-scores)[:k]\n",
    "        \n",
    "        for item_idx in top_items:\n",
    "            item_id = item_encoder.inverse_transform([item_idx])[0]\n",
    "            recommendations.append({\n",
    "                'user_id': user_id,\n",
    "                'item_id': item_id,\n",
    "                'score': scores[item_idx]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(recommendations)\n",
    "\n",
    "# Полный пайплайн LightGCN\n",
    "if __name__ == \"__main__\":\n",
    "    # Подготовка данных\n",
    "    train_data, test_data, user_encoder, item_encoder = prepare_data(filtered_transactions)\n",
    "    \n",
    "    num_users = len(user_encoder.classes_)\n",
    "    num_items = len(item_encoder.classes_)\n",
    "    \n",
    "    # Обучение LightGCN\n",
    "    model = train_lightgcn(train_data, num_users, num_items, epochs=100)\n",
    "    \n",
    "    # Генерация рекомендаций\n",
    "    recommendations = generate_recommendations(model, user_encoder, item_encoder)\n",
    "    \n",
    "    # Расчет метрик\n",
    "    metrics = RecommendationMetricsDf(recommendations, test_data, USER_ID, ITEM_ID)\n",
    "    print(f\"Precision@{N}: {metrics.precision_at_k(k=N):.4f}\")\n",
    "    print(f\"Recall@{N}: {metrics.recall_at_k(k=N):.4f}\")\n",
    "    print(f\"MAP@{N}: {metrics.map_at_k(k=N):.4f}\")\n",
    "    print(f\"NDCG@{N}: {metrics.ndcg_at_k(k=N):.4f}\")\n",
    "    print(f\"Coverage: {metrics.coverage(len(filtered_articles)):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb6a729",
   "metadata": {},
   "source": [
    "## NGCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "356fc8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:00<00:07,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 2.0135, Test AUC = 0.7124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [00:00<00:02, 11.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss = 1.9792, Test AUC = 0.7178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [00:00<00:01, 14.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss = 1.9491, Test AUC = 0.7238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [00:01<00:00, 15.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Loss = 1.9239, Test AUC = 0.7244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [00:01<00:00, 16.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Loss = 1.9182, Test AUC = 0.7346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:01<00:00, 17.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Loss = 1.8919, Test AUC = 0.7461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10: 0.0176\n",
      "Recall@10: 0.1710\n",
      "MAP@10: 0.0673\n",
      "NDCG@10: 0.0938\n",
      "Coverage: 14.22%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "class NGCFLayer(MessagePassing):\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        super().__init__(aggr='add')\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        # Weight matrices for message passing\n",
    "        self.W1 = nn.Linear(in_dim, out_dim)\n",
    "        self.W2 = nn.Linear(in_dim, out_dim)\n",
    "        \n",
    "        # LeakyReLU for non-linearity\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.W1.weight)\n",
    "        nn.init.xavier_uniform_(self.W2.weight)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        return self.propagate(edge_index, x=x)\n",
    "    \n",
    "    def message(self, x_j: torch.Tensor) -> torch.Tensor:\n",
    "        # Message computation with dropout\n",
    "        return self.dropout(x_j)\n",
    "    \n",
    "    def update(self, aggr_out: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        # NGCF combination rule\n",
    "        out = self.leaky_relu(self.W1(aggr_out)) + self.leaky_relu(self.W2(x))\n",
    "        return out\n",
    "\n",
    "class NGCF(nn.Module):\n",
    "    def __init__(self, num_users: int, num_items: int, \n",
    "                 embedding_dim: int = 64, \n",
    "                 layer_dims: List[int] = [64, 64, 64],\n",
    "                 dropout: float = 0.1,\n",
    "                 node_dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.layer_dims = layer_dims\n",
    "        self.n_layers = len(layer_dims)\n",
    "        self.dropout = dropout\n",
    "        self.node_dropout = node_dropout\n",
    "        \n",
    "        # User and item embeddings\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        # NGCF layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        input_dim = embedding_dim\n",
    "        for output_dim in layer_dims:\n",
    "            self.convs.append(NGCFLayer(input_dim, output_dim))\n",
    "            input_dim = output_dim\n",
    "        \n",
    "        # Prediction layer - удаляем, так как будем использовать dot product\n",
    "        # self.predict_layer = nn.Linear(layer_dims[-1], 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.item_embedding.weight)\n",
    "        # nn.init.xavier_uniform_(self.predict_layer.weight)\n",
    "    \n",
    "    def forward(self, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        # Node dropout for regularization\n",
    "        if self.training and self.node_dropout > 0:\n",
    "            mask = torch.rand(edge_index.size(1)) >= self.node_dropout\n",
    "            edge_index = edge_index[:, mask]\n",
    "        \n",
    "        # Initial embeddings\n",
    "        user_emb = self.user_embedding.weight\n",
    "        item_emb = self.item_embedding.weight\n",
    "        x = torch.cat([user_emb, item_emb], dim=0)\n",
    "        \n",
    "        # List to store embeddings at each layer\n",
    "        embeddings = [x]\n",
    "        \n",
    "        # Message passing through all layers\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            embeddings.append(x)\n",
    "        \n",
    "        # Combine embeddings from all layers\n",
    "        # Изменяем способ комбинирования эмбеддингов\n",
    "        final_embeddings = torch.mean(torch.stack(embeddings, dim=0), dim=0)\n",
    "        \n",
    "        # Split into users and items\n",
    "        user_final, item_final = torch.split(\n",
    "            final_embeddings, [self.num_users, self.num_items]\n",
    "        )\n",
    "        \n",
    "        # Calculate predictions using dot product\n",
    "        preds = torch.sigmoid(torch.mm(user_final, item_final.t()))\n",
    "        return preds\n",
    "\n",
    "def prepare_data(data: pd.DataFrame, \n",
    "                user_col: str = 'user_id',\n",
    "                time_col: str = 'time',\n",
    "                test_ratio: float = 0.2) -> tuple:\n",
    "    \"\"\"Подготовка данных с временным разделением для графовых сетей\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame с колонками user_id, item_id, time\n",
    "        user_col: название колонки с идентификаторами пользователей\n",
    "        time_col: название колонки с временными метками\n",
    "        test_ratio: доля тестовых данных\n",
    "    \n",
    "    Returns:\n",
    "        Кортеж (train_data, test_data, user_encoder, item_encoder)\n",
    "    \"\"\"\n",
    "    # Сортируем по пользователю и времени\n",
    "    data = data.sort_values([user_col, time_col])\n",
    "    \n",
    "    # Функция для разделения данных пользователя\n",
    "    def split_user_group(df):\n",
    "        split_idx = int(len(df) * (1 - test_ratio))\n",
    "        train = df.iloc[:split_idx]\n",
    "        test = df.iloc[split_idx:]\n",
    "        return train, test\n",
    "    \n",
    "    # Разделяем данные для каждого пользователя\n",
    "    grouped = data.groupby(user_col, group_keys=False)\n",
    "    train_data = grouped.apply(lambda x: split_user_group(x)[0])\n",
    "    test_data = grouped.apply(lambda x: split_user_group(x)[1])\n",
    "    \n",
    "    # Кодируем пользователей и товары (только на train)\n",
    "    user_encoder = LabelEncoder()\n",
    "    item_encoder = LabelEncoder()\n",
    "    \n",
    "    # Фитируем кодировщики на train данных\n",
    "    train_data['user_idx'] = user_encoder.fit_transform(train_data[user_col])\n",
    "    train_data['item_idx'] = item_encoder.fit_transform(train_data['item_id'])\n",
    "    \n",
    "    # Фильтруем тестовые данные (только известные пользователи и товары)\n",
    "    test_data = test_data[test_data[user_col].isin(user_encoder.classes_)]\n",
    "    test_data = test_data[test_data['item_id'].isin(item_encoder.classes_)]\n",
    "    \n",
    "    # Кодируем тестовые данные\n",
    "    test_data['user_idx'] = user_encoder.transform(test_data[user_col])\n",
    "    test_data['item_idx'] = item_encoder.transform(test_data['item_id'])\n",
    "    \n",
    "    # Проверяем, что в тестовых данных есть хотя бы одно взаимодействие\n",
    "    if len(test_data) == 0:\n",
    "        raise ValueError(\"Test data is empty after filtering. Check your data split.\")\n",
    "    \n",
    "    return train_data, test_data, user_encoder, item_encoder\n",
    "\n",
    "def train_ngcf(train_data: pd.DataFrame, num_users: int, num_items: int, \n",
    "              epochs: int = 100, batch_size: int = 1024) -> nn.Module:\n",
    "    # Создаем граф\n",
    "    edge_index = torch.tensor([\n",
    "        train_data['user_idx'].values,\n",
    "        train_data['item_idx'].values + num_users\n",
    "    ], dtype=torch.long)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = NGCF(num_users, num_items).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    criterion = nn.BCELoss()\n",
    "    edge_index = edge_index.to(device)\n",
    "    \n",
    "    # Негативное сэмплирование\n",
    "    def create_batch(data, batch_size=batch_size, neg_ratio=1):\n",
    "        users = data['user_idx'].values\n",
    "        items = data['item_idx'].values\n",
    "        \n",
    "        neg_users = np.repeat(users, neg_ratio)\n",
    "        neg_items = np.random.choice(num_items, len(users)*neg_ratio)\n",
    "        \n",
    "        all_users = np.concatenate([users, neg_users])\n",
    "        all_items = np.concatenate([items, neg_items])\n",
    "        labels = np.concatenate([\n",
    "            np.ones(len(users)),\n",
    "            np.zeros(len(users)*neg_ratio)\n",
    "        ])\n",
    "        \n",
    "        indices = np.random.permutation(len(all_users))\n",
    "        for i in range(0, len(indices), batch_size):\n",
    "            batch_idx = indices[i:i+batch_size]\n",
    "            yield (\n",
    "                torch.LongTensor(all_users[batch_idx]).to(device),\n",
    "                torch.LongTensor(all_items[batch_idx]).to(device),\n",
    "                torch.FloatTensor(labels[batch_idx]).to(device)\n",
    "            )\n",
    "    \n",
    "    # Обучение с ранней остановкой\n",
    "    best_auc = 0\n",
    "    patience = 5\n",
    "    no_improve = 0\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for users, items, labels in create_batch(train_data):\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(edge_index)[users, items]\n",
    "            loss = criterion(preds, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Оценка\n",
    "        if epoch % 5 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                preds = model(edge_index).cpu()\n",
    "                \n",
    "                test_users = test_data['user_idx'].unique()\n",
    "                if len(test_users) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                auc_scores = []\n",
    "                for user in test_users:\n",
    "                    pos_items = test_data[test_data['user_idx'] == user]['item_idx'].values\n",
    "                    if len(pos_items) == 0:\n",
    "                        continue\n",
    "                        \n",
    "                    neg_items = np.random.choice(num_items, min(99, num_items), replace=False)\n",
    "                    all_items = np.concatenate([pos_items, neg_items])\n",
    "                    labels = np.concatenate([np.ones(len(pos_items)), np.zeros(len(neg_items))])\n",
    "                    \n",
    "                    user_preds = preds[user, all_items].numpy()\n",
    "                    auc_scores.append(roc_auc_score(labels, user_preds))\n",
    "                \n",
    "                current_auc = np.mean(auc_scores) if auc_scores else 0.0\n",
    "                if current_auc > best_auc:\n",
    "                    best_auc = current_auc\n",
    "                    no_improve = 0\n",
    "                    torch.save(model.state_dict(), 'best_ngcf_model.pth')\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "                    if no_improve >= patience:\n",
    "                        print(\"Early stopping triggered\")\n",
    "                        break\n",
    "                \n",
    "                print(f'Epoch {epoch}: Loss = {total_loss:.4f}, Test AUC = {current_auc:.4f}')\n",
    "    \n",
    "    model.load_state_dict(torch.load('best_ngcf_model.pth'))\n",
    "    model.edge_index = edge_index  # Сохраняем edge_index в модели\n",
    "    return model\n",
    "\n",
    "def generate_recommendations(model: nn.Module, \n",
    "                           user_encoder: LabelEncoder, \n",
    "                           item_encoder: LabelEncoder, \n",
    "                           k: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"Генерация рекомендаций (без изменений)\"\"\"\n",
    "    model.eval()\n",
    "    num_users = len(user_encoder.classes_)\n",
    "    num_items = len(item_encoder.classes_)\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        preds = model(model.edge_index).cpu().numpy()\n",
    "    \n",
    "    recommendations = []\n",
    "    for user_idx in range(num_users):\n",
    "        user_id = user_encoder.inverse_transform([user_idx])[0]\n",
    "        scores = preds[user_idx]\n",
    "        top_items = np.argsort(-scores)[:k]\n",
    "        \n",
    "        for item_idx in top_items:\n",
    "            item_id = item_encoder.inverse_transform([item_idx])[0]\n",
    "            recommendations.append({\n",
    "                'user_id': user_id,\n",
    "                'item_id': item_id,\n",
    "                'score': scores[item_idx]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(recommendations)\n",
    "\n",
    "# Полный пайплайн NGCF\n",
    "if __name__ == \"__main__\":\n",
    "    # Подготовка данных\n",
    "    train_data, test_data, user_encoder, item_encoder = prepare_data(filtered_transactions)\n",
    "    \n",
    "    num_users = len(user_encoder.classes_)\n",
    "    num_items = len(item_encoder.classes_)\n",
    "    \n",
    "    # Обучение NGCF\n",
    "    model = train_ngcf(train_data, num_users, num_items, epochs=30)\n",
    "    \n",
    "    # Генерация рекомендаций\n",
    "    recommendations = generate_recommendations(model, user_encoder, item_encoder)\n",
    "    \n",
    "    # Расчет метрик\n",
    "    metrics = RecommendationMetricsDf(recommendations, test_data, USER_ID, ITEM_ID)\n",
    "    print(f\"Precision@{N}: {metrics.precision_at_k(k=N):.4f}\")\n",
    "    print(f\"Recall@{N}: {metrics.recall_at_k(k=N):.4f}\")\n",
    "    print(f\"MAP@{N}: {metrics.map_at_k(k=N):.4f}\")\n",
    "    print(f\"NDCG@{N}: {metrics.ndcg_at_k(k=N):.4f}\")\n",
    "    print(f\"Coverage: {metrics.coverage(len(filtered_articles)):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86931f5d",
   "metadata": {},
   "source": [
    "## GAT (Graph Attention Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ce1d790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:00<00:07,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 2.6904, Test AUC = 0.6695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [00:00<00:02, 11.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss = 2.6025, Test AUC = 0.6757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [00:00<00:00, 18.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss = 2.4825, Test AUC = 0.6854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [00:01<00:00, 15.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Loss = 2.4573, Test AUC = 0.7044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [00:01<00:00, 13.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Loss = 2.3598, Test AUC = 0.7115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:01<00:00, 15.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Loss = 2.3776, Test AUC = 0.7163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10: 0.0360\n",
      "Recall@10: 0.3490\n",
      "MAP@10: 0.2712\n",
      "NDCG@10: 0.2976\n",
      "Coverage: 38.89%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "class GATRecommender(nn.Module):\n",
    "    def __init__(self, num_users: int, num_items: int, \n",
    "                 embedding_dim: int = 64, \n",
    "                 hidden_dim: int = 128,\n",
    "                 heads: int = 4,\n",
    "                 dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # User and item embeddings\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        # GAT layers\n",
    "        self.conv1 = GATConv(\n",
    "            embedding_dim, \n",
    "            hidden_dim // heads, \n",
    "            heads=heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.conv2 = GATConv(\n",
    "            hidden_dim, \n",
    "            hidden_dim, \n",
    "            heads=1,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.item_embedding.weight)\n",
    "    \n",
    "    def forward(self, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        # Get initial embeddings\n",
    "        user_emb = self.user_embedding(torch.arange(self.num_users, device=edge_index.device))\n",
    "        item_emb = self.item_embedding(torch.arange(self.num_items, device=edge_index.device))\n",
    "        x = torch.cat([user_emb, item_emb], dim=0)\n",
    "        \n",
    "        # First GAT layer\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Second GAT layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        # Split into users and items\n",
    "        user_final = x[:self.num_users]\n",
    "        item_final = x[self.num_users:]\n",
    "        \n",
    "        # Calculate predictions using dot product + sigmoid\n",
    "        return torch.sigmoid(torch.mm(user_final, item_final.t()))\n",
    "\n",
    "def prepare_data(data: pd.DataFrame, \n",
    "                user_col: str = 'user_id',\n",
    "                time_col: str = 'time',\n",
    "                test_ratio: float = 0.2) -> tuple:\n",
    "    \"\"\"Подготовка данных с временным разделением для графовых сетей\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame с колонками user_id, item_id, time\n",
    "        user_col: название колонки с идентификаторами пользователей\n",
    "        time_col: название колонки с временными метками\n",
    "        test_ratio: доля тестовых данных\n",
    "    \n",
    "    Returns:\n",
    "        Кортеж (train_data, test_data, user_encoder, item_encoder)\n",
    "    \"\"\"\n",
    "    # Сортируем по пользователю и времени\n",
    "    data = data.sort_values([user_col, time_col])\n",
    "    \n",
    "    # Функция для разделения данных пользователя\n",
    "    def split_user_group(df):\n",
    "        split_idx = int(len(df) * (1 - test_ratio))\n",
    "        train = df.iloc[:split_idx]\n",
    "        test = df.iloc[split_idx:]\n",
    "        return train, test\n",
    "    \n",
    "    # Разделяем данные для каждого пользователя\n",
    "    grouped = data.groupby(user_col, group_keys=False)\n",
    "    train_data = grouped.apply(lambda x: split_user_group(x)[0])\n",
    "    test_data = grouped.apply(lambda x: split_user_group(x)[1])\n",
    "    \n",
    "    # Кодируем пользователей и товары (только на train)\n",
    "    user_encoder = LabelEncoder()\n",
    "    item_encoder = LabelEncoder()\n",
    "    \n",
    "    # Фитируем кодировщики на train данных\n",
    "    train_data['user_idx'] = user_encoder.fit_transform(train_data[user_col])\n",
    "    train_data['item_idx'] = item_encoder.fit_transform(train_data['item_id'])\n",
    "    \n",
    "    # Фильтруем тестовые данные (только известные пользователи и товары)\n",
    "    test_data = test_data[test_data[user_col].isin(user_encoder.classes_)]\n",
    "    test_data = test_data[test_data['item_id'].isin(item_encoder.classes_)]\n",
    "    \n",
    "    # Кодируем тестовые данные\n",
    "    test_data['user_idx'] = user_encoder.transform(test_data[user_col])\n",
    "    test_data['item_idx'] = item_encoder.transform(test_data['item_id'])\n",
    "    \n",
    "    # Проверяем, что в тестовых данных есть хотя бы одно взаимодействие\n",
    "    if len(test_data) == 0:\n",
    "        raise ValueError(\"Test data is empty after filtering. Check your data split.\")\n",
    "    \n",
    "    return train_data, test_data, user_encoder, item_encoder\n",
    "\n",
    "def train_gat(train_data: pd.DataFrame, num_users: int, num_items: int, \n",
    "             epochs: int = 100, batch_size: int = 1024) -> nn.Module:\n",
    "    # Создаем граф\n",
    "    edge_index = torch.tensor([\n",
    "        train_data['user_idx'].values,\n",
    "        train_data['item_idx'].values + num_users\n",
    "    ], dtype=torch.long)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = GATRecommender(\n",
    "        num_users, \n",
    "        num_items,\n",
    "        embedding_dim=64,\n",
    "        hidden_dim=128,\n",
    "        heads=4,\n",
    "        dropout=0.2\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    criterion = nn.BCELoss()\n",
    "    edge_index = edge_index.to(device)\n",
    "    \n",
    "    # Негативное сэмплирование\n",
    "    def create_batch(data, batch_size=batch_size, neg_ratio=2):\n",
    "        users = data['user_idx'].values\n",
    "        items = data['item_idx'].values\n",
    "        \n",
    "        neg_users = np.repeat(users, neg_ratio)\n",
    "        neg_items = np.random.choice(num_items, len(users)*neg_ratio)\n",
    "        \n",
    "        all_users = np.concatenate([users, neg_users])\n",
    "        all_items = np.concatenate([items, neg_items])\n",
    "        labels = np.concatenate([\n",
    "            np.ones(len(users)),\n",
    "            np.zeros(len(users)*neg_ratio)\n",
    "        ])\n",
    "        \n",
    "        indices = np.random.permutation(len(all_users))\n",
    "        for i in range(0, len(indices), batch_size):\n",
    "            batch_idx = indices[i:i+batch_size]\n",
    "            yield (\n",
    "                torch.LongTensor(all_users[batch_idx]).to(device),\n",
    "                torch.LongTensor(all_items[batch_idx]).to(device),\n",
    "                torch.FloatTensor(labels[batch_idx]).to(device)\n",
    "            )\n",
    "    \n",
    "    # Обучение с ранней остановкой\n",
    "    best_auc = 0\n",
    "    patience = 5\n",
    "    no_improve = 0\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for users, items, labels in create_batch(train_data):\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(edge_index)[users, items]\n",
    "            loss = criterion(preds, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Оценка\n",
    "        if epoch % 5 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                preds = model(edge_index).cpu()\n",
    "                \n",
    "                test_users = test_data['user_idx'].unique()\n",
    "                if len(test_users) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                auc_scores = []\n",
    "                for user in test_users:\n",
    "                    pos_items = test_data[test_data['user_idx'] == user]['item_idx'].values\n",
    "                    if len(pos_items) == 0:\n",
    "                        continue\n",
    "                        \n",
    "                    neg_items = np.random.choice(num_items, min(99, num_items), replace=False)\n",
    "                    all_items = np.concatenate([pos_items, neg_items])\n",
    "                    labels = np.concatenate([np.ones(len(pos_items)), np.zeros(len(neg_items))])\n",
    "                    \n",
    "                    user_preds = preds[user, all_items].numpy()\n",
    "                    auc_scores.append(roc_auc_score(labels, user_preds))\n",
    "                \n",
    "                current_auc = np.mean(auc_scores) if auc_scores else 0.0\n",
    "                if current_auc > best_auc:\n",
    "                    best_auc = current_auc\n",
    "                    no_improve = 0\n",
    "                    torch.save(model.state_dict(), 'best_gat_model.pth')\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "                    if no_improve >= patience:\n",
    "                        print(\"Early stopping triggered\")\n",
    "                        break\n",
    "                \n",
    "                print(f'Epoch {epoch}: Loss = {total_loss:.4f}, Test AUC = {current_auc:.4f}')\n",
    "    \n",
    "    model.load_state_dict(torch.load('best_gat_model.pth'))\n",
    "    model.edge_index = edge_index\n",
    "    return model\n",
    "\n",
    "def generate_recommendations(model: nn.Module, \n",
    "                           user_encoder: LabelEncoder, \n",
    "                           item_encoder: LabelEncoder, \n",
    "                           k: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"Генерация рекомендаций (аналогично предыдущим реализациям)\"\"\"\n",
    "    model.eval()\n",
    "    num_users = len(user_encoder.classes_)\n",
    "    num_items = len(item_encoder.classes_)\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        preds = model(model.edge_index).cpu().numpy()\n",
    "    \n",
    "    recommendations = []\n",
    "    for user_idx in range(num_users):\n",
    "        user_id = user_encoder.inverse_transform([user_idx])[0]\n",
    "        scores = preds[user_idx]\n",
    "        top_items = np.argsort(-scores)[:k]\n",
    "        \n",
    "        for item_idx in top_items:\n",
    "            item_id = item_encoder.inverse_transform([item_idx])[0]\n",
    "            recommendations.append({\n",
    "                'user_id': user_id,\n",
    "                'item_id': item_id,\n",
    "                'score': scores[item_idx]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(recommendations)\n",
    "\n",
    "# Полный пайплайн GAT\n",
    "if __name__ == \"__main__\":\n",
    "    # Подготовка данных\n",
    "    train_data, test_data, user_encoder, item_encoder = prepare_data(filtered_transactions)\n",
    "    \n",
    "    num_users = len(user_encoder.classes_)\n",
    "    num_items = len(item_encoder.classes_)\n",
    "    \n",
    "    # Обучение GAT\n",
    "    model = train_gat(train_data, num_users, num_items, epochs=30)\n",
    "    \n",
    "    # Генерация рекомендаций\n",
    "    recommendations = generate_recommendations(model, user_encoder, item_encoder)\n",
    "    \n",
    "    # Расчет метрик\n",
    "    metrics = RecommendationMetricsDf(recommendations, test_data, USER_ID, ITEM_ID)\n",
    "    print(f\"Precision@{N}: {metrics.precision_at_k(k=N):.4f}\")\n",
    "    print(f\"Recall@{N}: {metrics.recall_at_k(k=N):.4f}\")\n",
    "    print(f\"MAP@{N}: {metrics.map_at_k(k=N):.4f}\")\n",
    "    print(f\"NDCG@{N}: {metrics.ndcg_at_k(k=N):.4f}\")\n",
    "    print(f\"Coverage: {metrics.coverage(len(filtered_articles)):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc783a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
